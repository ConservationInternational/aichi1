control = list(verbose=T))
rules_biodiv_ame <- apriori(arm_data_ame,
appearance = list(rhs=c("WH.invasive species=No change")),
control = list(verbose=T))
all_db_diff[which(all_db_diff$GT.biodiversity=="Increase"),]
length(which(all_db_diff$GT.biodiversity=="Increase"))
arm_data_eur<-all_db_diff[which(all_db_diff$continent=="Europe"),c(1:66)]
rules_biodiv_eur <- apriori(arm_data_eur,
appearance = list(rhs=c("GT.extinction=Increase")),
control = list(verbose=T))
#Calculate association rules
names(all_db_diff)
rules_biodiv_eur <- apriori(arm_data_eur,
appearance = list(rhs=c("GT.endemic species=Increase")),
control = list(verbose=T))
rules_biodiv_eur <- apriori(arm_data_eur,
appearance = list(rhs=c("GT.deforestation=Increase")),
control = list(verbose=T))
rules_biodiv_eur.sorted <- sort(rules_biodiv_eur, by="lift")
inspect(head(sort(rules_biodiv_eur.sorted), n=10))
rules_biodiv_eur <- apriori(arm_data_eur,
appearance = list(rhs=c("GT.sustainability=Increase")),
control = list(verbose=T))
rules_biodiv_eur <- apriori(arm_data_eur,
appearance = list(rhs=c("GT.ecosystem service=Increase")),
control = list(verbose=T))
rules_biodiv_eur <- apriori(arm_data_eur,
appearance = list(rhs=c("GT.extinction=Increase")),
control = list(verbose=T))
rules_biodiv_eur <- apriori(arm_data_eur,
appearance = list(rhs=c("GT.invasive species=Increase")),
control = list(verbose=T))
rules_biodiv_eur <- apriori(arm_data_eur,
appearance = list(rhs=c("GT.desertification=Increase")),
control = list(verbose=T))
arm_data_eur<-all_db_diff[which(all_db_diff$continent=="Asia"),c(1:66)]
rules_biodiv_eur <- apriori(arm_data_eur,
appearance = list(rhs=c("GT.deforestation=Increase")),
control = list(verbose=T))
rules_biodiv_eur <- apriori(arm_data_eur,
appearance = list(rhs=c("GT.biodiversity=Increase")),
control = list(verbose=T))
rules_biodiv_eur.sorted <- sort(rules_biodiv_eur, by="lift")
####Open data files
#Trends
webhose_df<-read.csv("Data/WEBHOSE-ALL.csv")
webhose_df<-webhose_df[which(webhose_df$set=="issue"),c(1,2,4,7)]
head(webhose_df)
str(webhose_df)
#Summarize data for the sampling period
webhose_df_sum<-webhose_df %>% group_by(country,issue) %>% summarise(rate = mean(rate))
#Convert contry names to character
webhose_df_sum$country<-as.character(webhose_df_sum$country)
#Convert database from wide to long format
webhose_sum_wide<-spread(webhose_df_sum,issue,rate)
webhose_sum_wide
unique(webhose_df_sum)
unique(webhose_sum_wide)
unique(webhose_sum_wide$country)
rowsum(webhose_sum_wide[,-1])
rowSums(webhose_sum_wide[,-1])
which(rowSums(webhose_sum_wide[,-1])==0)
#Remove rows with no articles
webhose_sum_wide<-webhose_sum_wide[which(rowSums(webhose_sum_wide[,-1])>0),]
####Open data files
#Trends
webhose_df<-read.csv("Data/WEBHOSE-ALL.csv")
head(webhose_df)
webhose_df<-subset(webhose_df,webhose_df$issue!="all")
head(webhose_df)
webhose_df<-webhose_df[,c(1:2,4:7)]
head(webhose_df)
str(webhose_df)
#Summarize baseline data for the sampling period
webhose_df_sum<-webhose_df %>% group_by(country,issue) %>% summarise(rate = sum(baseline))
webhose_df_sum
#Summarize baseline data for the sampling period
webhose_df_sum<-webhose_df %>% group_by(country) %>% summarise(rate = sum(baseline))
webhose_df_sum
webhose_df_sum$rate
median(webhose_df_sum$rate)
hist(webhose_df_sum$rate)
summary(webhose_df_sum$rate)
ctr_vol<-webhose_df_sum[which(webhose_df_sum$rate>=10000),1]
ctr_vol
webhose_df_vol<-webhose_df[webhose_df$country %in% ctr_vol,]
webhose_df$country %in% ctr_vol
ctr_vol
webhose_df$country
#Filter database for countries with more than 10000 baseline articles
ctr_vol<-as.character(webhose_df_sum[which(webhose_df_sum$rate>=10000),1])
webhose_df$country %in% ctr_vol
summary(webhose_df$country %in% ctr_vol)
summary(as.character(webhose_df$country) %in% ctr_vol)
#Filter database for countries with more than 10000 baseline articles
ctr_vol<-as.vector(webhose_df_sum[which(webhose_df_sum$rate>=10000),1])
ctr_vol
#Filter database for countries with more than 10000 baseline articles
ctr_vol<-as.list(webhose_df_sum[which(webhose_df_sum$rate>=10000),1])
ctr_vol
webhose_df$country %in% ctr_vol
head(webhose_df$country)
webhose_df$country<-as.character(webhose_df$country)
webhose_df$country %in% ctr_vol
#Filter database for countries with more than 10000 baseline articles
ctr_vol<-as.character(as.list(webhose_df_sum[which(webhose_df_sum$rate>=10000),1]))
ctr_vol
#Filter database for countries with more than 10000 baseline articles
ctr_vol<-unlist(webhose_df_sum[which(webhose_df_sum$rate>=10000),1])
ctr_vol
#Summarize baseline data for the sampling period
webhose_df_sum<-as.data.frame(webhose_df %>% group_by(country) %>% summarise(rate = sum(baseline)))
#Filter database for countries with more than 10000 baseline articles
ctr_vol<-webhose_df_sum[which(webhose_df_sum$rate>=10000),1]
ctr_vol
webhose_df$country<-as.character(webhose_df$country)
webhose_df$country %in% ctr_vol
webhose_df_vol<-webhose_df[webhose_df$country %in% ctr_vol,]
head(webhose_df)
webhose_df[1:10,]
#Get largest rate for each issue
webhose_df_sum<-as.data.frame(webhose_df_vol %>% group_by(issue) %>% summarise(top = max(rate)))
#Get largest rate for each issue
webhose_df_maxrate<-as.data.frame(webhose_df_vol %>% group_by(issue) %>% summarise(top = max(rate)))
webhose_df_maxrate
#Merge data and calculate relative volume
webhose_merge<-left_join(webhose_df_vol,webhose_df_maxrate,by=("issue"))
webhose_merge
webhose_merge$rvol<-webhose_merge$rate/webhose_merge$top
webhose_merge
webhose_merge$rvol<-(webhose_merge$rate*100)/webhose_merge$top
webhose_merge
#Convert database from wide to long format
webhose_wide<-spread(webhose_merge[,c(1:3,8)],issue,rvol)
webhose_wide
webhose_sum<-as.data.frame(webhose_merge %>% group_by(country,issue) %>% summarise(mean_rvol = mean(rvol)))
webhose_sum
#Convert database from wide to long format
webhose_wide<-spread(webhose_sum,issue,mean_rvol)
webhose_wide
#Convert database from wide to long format
webhose_sum_wide<-spread(webhose_sum,issue,mean_rvol)
#remove countries where search interest never peaked above 25 - this removes coutnries with few data
webhose_wide<-webhose_sum_wide[apply(webhose_sum_wide[,2:23],1,function(x) any(x>10)),]
head(webhose_sum_wide)
#remove countries where search interest never peaked above 25 - this removes coutnries with few data
webhose_wide<-webhose_sum_wide[apply(webhose_sum_wide[,2:23],1,function(x) any(x>5)),]
head(webhose_sum_wide)
#Associate countries with continents and create a data frame with that information
unique_ctr_sum<-webhose_sum_wide$country
country_data_sum<-data.frame(ctr_names_iso2=unique_ctr_sum,
ctr_names_full=countrycode(unique_ctr_sum, 'iso2c', 'country.name'),
continent=countrycode(unique_ctr_sum, 'iso2c', 'continent'),
stringsAsFactors=F)
country_data_sum$continent<-as.factor(country_data_sum$continent)
country_data_sum$continent
#Calculate PCA
res.pca <- prcomp(webhose_sum_wide[,c(2:23)], scale = TRUE)
rownames(res.pca$x)<-unique_ctr_sum
cols<-brewer.pal(5, "Spectral")
#Country plot
ctr.plot<-fviz_pca_ind(res.pca,
geom.ind = c("point","text"),
pointshape = 21,
pointsize = "cos2",
fill.ind = country_data_sum$continent,col.ind="black",invisible = "quali",
legend.title = list(fill = "Continent", size = "Representation"),
repel = TRUE)+
ggpubr::fill_palette("Spectral")+
theme(legend.position = "bottom")+
guides(fill = guide_legend(override.aes = list(size = 5)))
#Load relevant packages for data extraction
library(tidyverse)
library(ggplot2)
library(tidygraph)
library(ggraph)
library(countrycode)
library(RColorBrewer)
library(factoextra)
library(ggpubr)
library(cowplot)
library(ggcorrplot)
#Calculate PCA
res.pca <- prcomp(webhose_sum_wide[,c(2:23)], scale = TRUE)
rownames(res.pca$x)<-unique_ctr_sum
cols<-brewer.pal(5, "Spectral")
#Country plot
ctr.plot<-fviz_pca_ind(res.pca,
geom.ind = c("point","text"),
pointshape = 21,
pointsize = "cos2",
fill.ind = country_data_sum$continent,col.ind="black",invisible = "quali",
legend.title = list(fill = "Continent", size = "Representation"),
repel = TRUE)+
ggpubr::fill_palette("Spectral")+
theme(legend.position = "bottom")+
guides(fill = guide_legend(override.aes = list(size = 5)))
ctr.plot<-ggpar(ctr.plot,
title = "Principal Component Analysis - Google webhose country data",
xlab = "PC1 (26.6%)", ylab = "PC2 (12.2%)")
ctr.plot
####Open data files
#Trends
webhose_df<-read.csv("Data/WEBHOSE-ALL.csv")
webhose_df<-subset(webhose_df,webhose_df$issue!="all")
webhose_df<-webhose_df[,c(1:2,4:7)]
head(webhose_df)
str(webhose_df)
#Summarize baseline data for the sampling period
webhose_df_sum<-as.data.frame(webhose_df %>% group_by(country) %>% summarise(rate = sum(baseline)))
#Filter database for countries with more than 10000 baseline articles
ctr_vol<-webhose_df_sum[which(webhose_df_sum$rate>=25000),1]
webhose_df$country<-as.character(webhose_df$country)
webhose_df_vol<-webhose_df[webhose_df$country %in% ctr_vol,]
#Get largest rate for each issue
webhose_df_maxrate<-as.data.frame(webhose_df_vol %>% group_by(issue) %>% summarise(top = max(rate)))
#Merge data and calculate relative volume
webhose_merge<-left_join(webhose_df_vol,webhose_df_maxrate,by=("issue"))
webhose_merge$rvol<-(webhose_merge$rate*100)/webhose_merge$top
webhose_sum<-as.data.frame(webhose_merge %>% group_by(country,issue) %>% summarise(mean_rvol = mean(rvol)))
#Convert database from wide to long format
webhose_sum_wide<-spread(webhose_sum,issue,mean_rvol)
#remove countries where search interest never peaked above 25 - this removes coutnries with few data
webhose_wide<-webhose_sum_wide[apply(webhose_sum_wide[,2:23],1,function(x) any(x>5)),]
head(webhose_sum_wide)
#Associate countries with continents and create a data frame with that information
unique_ctr_sum<-webhose_sum_wide$country
country_data_sum<-data.frame(ctr_names_iso2=unique_ctr_sum,
ctr_names_full=countrycode(unique_ctr_sum, 'iso2c', 'country.name'),
continent=countrycode(unique_ctr_sum, 'iso2c', 'continent'),
stringsAsFactors=F)
country_data_sum$continent<-as.factor(country_data_sum$continent)
#Calculate PCA
res.pca <- prcomp(webhose_sum_wide[,c(2:23)], scale = TRUE)
rownames(res.pca$x)<-unique_ctr_sum
cols<-brewer.pal(5, "Spectral")
#Country plot
ctr.plot<-fviz_pca_ind(res.pca,
geom.ind = c("point","text"),
pointshape = 21,
pointsize = "cos2",
fill.ind = country_data_sum$continent,col.ind="black",invisible = "quali",
legend.title = list(fill = "Continent", size = "Representation"),
repel = TRUE)+
ggpubr::fill_palette("Spectral")+
theme(legend.position = "bottom")+
guides(fill = guide_legend(override.aes = list(size = 5)))
ctr.plot<-ggpar(ctr.plot,
title = "Principal Component Analysis - Google webhose country data",
xlab = "PC1 (26.6%)", ylab = "PC2 (12.2%)")
ctr.plot
####Open data files
#Trends
webhose_df<-read.csv("Data/WEBHOSE-ALL.csv")
webhose_df<-subset(webhose_df,webhose_df$issue!="all")
webhose_df<-webhose_df[,c(1:2,4:7)]
head(webhose_df)
str(webhose_df)
#Summarize baseline data for the sampling period
webhose_df_sum<-as.data.frame(webhose_df %>% group_by(country) %>% summarise(rate = sum(baseline)))
#Filter database for countries with more than 10000 baseline articles
ctr_vol<-webhose_df_sum[which(webhose_df_sum$rate>=50000),1]
which(webhose_df_sum$rate>=50000)
####Open data files
#Trends
webhose_df<-read.csv("Data/WEBHOSE-ALL.csv")
webhose_df<-subset(webhose_df,webhose_df$issue!="all")
webhose_df<-webhose_df[,c(1:2,4:7)]
webhose_df$country<-as.character(webhose_df$country)
head(webhose_df)
str(webhose_df)
#Summarize baseline data for the sampling period
webhose_df_sum<-as.data.frame(webhose_df %>% group_by(country) %>% summarise(rate = sum(baseline)))
#Filter database for countries with more than 10000 baseline articles
ctr_vol<-webhose_df_sum[which(webhose_df_sum$rate>=25000),1]
webhose_df_vol<-webhose_df[webhose_df$country %in% ctr_vol,]
#Get largest rate for each issue
webhose_df_maxrate<-as.data.frame(webhose_df_vol %>% group_by(issue) %>% summarise(top = max(rate)))
#Merge data and calculate relative volume
webhose_merge<-left_join(webhose_df_vol,webhose_df_maxrate,by=("issue"))
webhose_merge$rvol<-(webhose_merge$rate*100)/webhose_merge$top
webhose_sum<-as.data.frame(webhose_merge %>% group_by(country,issue) %>% summarise(mean_rvol = mean(rvol)))
#Convert database from wide to long format
webhose_sum_wide<-spread(webhose_sum,issue,mean_rvol)
#remove countries where search interest never peaked above 25 - this removes coutnries with few data
webhose_wide<-webhose_sum_wide[apply(webhose_sum_wide[,2:23],1,function(x) any(x>5)),]
head(webhose_sum_wide)
#remove countries where search interest never peaked above 25 - this removes coutnries with few data
webhose_wide<-webhose_sum_wide[apply(webhose_sum_wide[,2:23],1,function(x) any(x>10)),]
####Open data files
#Trends
webhose_df<-read.csv("Data/WEBHOSE-ALL.csv")
webhose_df<-subset(webhose_df,webhose_df$issue!="all")
webhose_df<-webhose_df[,c(1:2,4:7)]
webhose_df$country<-as.character(webhose_df$country)
head(webhose_df)
str(webhose_df)
#Summarize baseline data for the sampling period
webhose_df_sum<-as.data.frame(webhose_df %>% group_by(country) %>% summarise(rate = sum(baseline)))
#Filter database for countries with more than 10000 baseline articles
ctr_vol<-webhose_df_sum[which(webhose_df_sum$rate>=50000),1]
webhose_df_vol<-webhose_df[webhose_df$country %in% ctr_vol,]
#Get largest rate for each issue
webhose_df_maxrate<-as.data.frame(webhose_df_vol %>% group_by(issue) %>% summarise(top = max(rate)))
#Merge data and calculate relative volume
webhose_merge<-left_join(webhose_df_vol,webhose_df_maxrate,by=("issue"))
webhose_merge$rvol<-(webhose_merge$rate*100)/webhose_merge$top
webhose_sum<-as.data.frame(webhose_merge %>% group_by(country,issue) %>% summarise(mean_rvol = mean(rvol)))
#Convert database from wide to long format
webhose_sum_wide<-spread(webhose_sum,issue,mean_rvol)
#remove countries where search interest never peaked above 25 - this removes coutnries with few data
webhose_wide<-webhose_sum_wide[apply(webhose_sum_wide[,2:23],1,function(x) any(x>10)),]
#remove countries where search interest never peaked above 25 - this removes coutnries with few data
webhose_wide<-webhose_sum_wide[apply(webhose_sum_wide[,2:23],1,function(x) any(x>5)),]
head(webhose_sum_wide)
#Associate countries with continents and create a data frame with that information
unique_ctr_sum<-webhose_sum_wide$country
country_data_sum<-data.frame(ctr_names_iso2=unique_ctr_sum,
ctr_names_full=countrycode(unique_ctr_sum, 'iso2c', 'country.name'),
continent=countrycode(unique_ctr_sum, 'iso2c', 'continent'),
stringsAsFactors=F)
country_data_sum$continent<-as.factor(country_data_sum$continent)
#Calculate PCA
res.pca <- prcomp(webhose_sum_wide[,c(2:23)], scale = TRUE)
rownames(res.pca$x)<-unique_ctr_sum
cols<-brewer.pal(5, "Spectral")
#Country plot
ctr.plot<-fviz_pca_ind(res.pca,
geom.ind = c("point","text"),
pointshape = 21,
pointsize = "cos2",
fill.ind = country_data_sum$continent,col.ind="black",invisible = "quali",
legend.title = list(fill = "Continent", size = "Representation"),
repel = TRUE)+
ggpubr::fill_palette("Spectral")+
theme(legend.position = "bottom")+
guides(fill = guide_legend(override.aes = list(size = 5)))
ctr.plot<-ggpar(ctr.plot,
title = "Principal Component Analysis - Google webhose country data",
xlab = "PC1 (26.6%)", ylab = "PC2 (12.2%)")
ctr.plot
####Open data files
#Trends
webhose_df<-read.csv("Data/WEBHOSE-ALL.csv")
webhose_df<-subset(webhose_df,webhose_df$issue!="all")
webhose_df<-webhose_df[,c(1:2,4:7)]
webhose_df$country<-as.character(webhose_df$country)
head(webhose_df)
str(webhose_df)
#Summarize baseline data for the sampling period
webhose_df_sum<-as.data.frame(webhose_df %>% group_by(country) %>% summarise(rate = sum(baseline)))
rm(list=ls())
####Open data files
#Trends
webhose_df<-read.csv("Data/WEBHOSE-ALL.csv")
webhose_df<-subset(webhose_df,webhose_df$issue!="all")
webhose_df<-webhose_df[,c(1:2,4:7)]
webhose_df$country<-as.character(webhose_df$country)
head(webhose_df)
str(webhose_df)
#Summarize baseline data for the sampling period
webhose_df_sum<-as.data.frame(webhose_df %>% group_by(country) %>% summarise(rate = sum(baseline)))
#Filter database for countries with more than 10000 baseline articles
ctr_vol<-webhose_df_sum[which(webhose_df_sum$rate>=100000),1]
webhose_df_vol<-webhose_df[webhose_df$country %in% ctr_vol,]
#Filter database for countries with more than 10000 baseline articles
ctr_vol<-webhose_df_sum[which(webhose_df_sum$rate>=50000),1]
webhose_df_vol<-webhose_df[webhose_df$country %in% ctr_vol,]
#Get largest rate for each issue
webhose_df_maxrate<-as.data.frame(webhose_df_vol %>% group_by(issue) %>% summarise(top = max(rate)))
#Merge data and calculate relative volume
webhose_merge<-left_join(webhose_df_vol,webhose_df_maxrate,by=("issue"))
webhose_merge$rvol<-(webhose_merge$rate*100)/webhose_merge$top
webhose_sum<-as.data.frame(webhose_merge %>% group_by(country,issue) %>% summarise(mean_rvol = mean(rvol)))
#Convert database from wide to long format
webhose_sum_wide<-spread(webhose_sum,issue,mean_rvol)
#remove countries where search interest never peaked above 25 - this removes coutnries with few data
webhose_wide<-webhose_sum_wide[apply(webhose_sum_wide[,2:23],1,function(x) any(x>5)),]
head(webhose_sum_wide)
#Associate countries with continents and create a data frame with that information
unique_ctr_sum<-webhose_sum_wide$country
country_data_sum<-data.frame(ctr_names_iso2=unique_ctr_sum,
ctr_names_full=countrycode(unique_ctr_sum, 'iso2c', 'country.name'),
continent=countrycode(unique_ctr_sum, 'iso2c', 'continent'),
stringsAsFactors=F)
country_data_sum$continent<-as.factor(country_data_sum$continent)
#Calculate PCA
res.pca <- prcomp(webhose_sum_wide[,c(2:23)], scale = TRUE)
rownames(res.pca$x)<-unique_ctr_sum
cols<-brewer.pal(5, "Spectral")
#Country plot
ctr.plot<-fviz_pca_ind(res.pca,
geom.ind = c("point","text"),
pointshape = 21,
pointsize = "cos2",
fill.ind = country_data_sum$continent,col.ind="black",invisible = "quali",
legend.title = list(fill = "Continent", size = "Representation"),
repel = TRUE)+
ggpubr::fill_palette("Spectral")+
theme(legend.position = "bottom")+
guides(fill = guide_legend(override.aes = list(size = 5)))
ctr.plot<-ggpar(ctr.plot,
title = "Principal Component Analysis - Google webhose country data",
xlab = "PC1 (26.6%)", ylab = "PC2 (12.2%)")
ctr.plot
#Variable plot
var.plot<-fviz_pca_var(res.pca, col.var = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
legend.title = "Contribution",repel=T,labelsize = 4)+
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_rect(fill="white"))
var.plot<-ggpar(var.plot,
title = "Variable contribution",
xlab = "PC1 (26.6%)", ylab = "PC2 (12.2%)")
var.plot
#Build multipart figure
figure <- ggdraw() +
draw_plot(ctr.plot, x = 0, y = 0, width = .82, height = 1) +
draw_plot(var.plot, x = .5, y = .35, width = 0.6, height = 0.65)
figure
ctr.plot
#Build multipart figure
figure <- ggdraw() +
draw_plot(ctr.plot, x = 0, y = 0, width = .82, height = 1) +
draw_plot(var.plot, x = .5, y = 0, width = 0.6, height = 0.65)
figure
#Build multipart figure
figure <- ggdraw() +
draw_plot(ctr.plot, x = 0, y = 0, width = .82, height = 1) +
draw_plot(var.plot, x = .5, y = .1, width = 0.6, height = 0.65)
figure
#Save plot
ggsave("Figure1_webh.tiff",figure,scale=1.2,width=25,height=12,units="cm",dpi=300)
#Calculate PCA
res.pca <- prcomp(webhose_sum_wide[,c(2:23)], scale = TRUE)
rownames(res.pca$x)<-unique_ctr_sum
cols<-brewer.pal(5, "Spectral")
#Country plot
ctr.plot<-fviz_pca_ind(res.pca,
geom.ind = c("point","text"),
pointshape = 21,
pointsize = "cos2",
fill.ind = country_data_sum$continent,col.ind="black",invisible = "quali",
legend.title = list(fill = "Continent", size = "Representation"),
repel = TRUE)+
ggpubr::fill_palette("Spectral")+
theme(legend.position = "bottom")+
guides(fill = guide_legend(override.aes = list(size = 5)))
ctr.plot<-ggpar(ctr.plot,
title = "Principal Component Analysis - Webhose country data",
xlab = "PC1 (26.6%)", ylab = "PC2 (12.2%)")
ctr.plot
#Variable plot
var.plot<-fviz_pca_var(res.pca, col.var = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
legend.title = "Contribution",repel=T,labelsize = 4)+
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_rect(fill="white"))
var.plot<-ggpar(var.plot,
title = "Variable contribution",
xlab = "PC1 (26.6%)", ylab = "PC2 (12.2%)")
var.plot
#Build multipart figure
figure <- ggdraw() +
draw_plot(ctr.plot, x = 0, y = 0, width = .82, height = 1) +
draw_plot(var.plot, x = .5, y = .1, width = 0.6, height = 0.65)
figure
#Save plot
ggsave("Figure1_webh.tiff",figure,scale=1.2,width=25,height=12,units="cm",dpi=300)
#Variable plot
var.plot<-fviz_pca_var(res.pca, col.var = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
legend.title = "Contribution",repel=T,labelsize = 4)+
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_rect(fill="white"))
var.plot
#Calculate PCA
res.pca <- prcomp(webhose_sum_wide[,c(2:23)], scale = TRUE)
rownames(res.pca$x)<-unique_ctr_sum
cols<-brewer.pal(5, "Spectral")
#Country plot
ctr.plot<-fviz_pca_ind(res.pca,
geom.ind = c("point","text"),
pointshape = 21,
pointsize = "cos2",
fill.ind = country_data_sum$continent,col.ind="black",invisible = "quali",
legend.title = list(fill = "Continent", size = "Representation"),
repel = TRUE)+
ggpubr::fill_palette("Spectral")+
theme(legend.position = "bottom")+
guides(fill = guide_legend(override.aes = list(size = 5)))
ctr.plot<-ggpar(ctr.plot,
title = "Principal Component Analysis - Webhose country data",
xlab = "PC1 (21.6%)", ylab = "PC2 (9.5%)")
ctr.plot
#Variable plot
var.plot<-fviz_pca_var(res.pca, col.var = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
legend.title = "Contribution",repel=T,labelsize = 4)+
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_rect(fill="white"))
var.plot<-ggpar(var.plot,
title = "Variable contribution",
xlab = "PC1 (21.6%)", ylab = "PC2 (9.5%)")
var.plot
#Build multipart figure
figure <- ggdraw() +
draw_plot(ctr.plot, x = 0, y = 0, width = .82, height = 1) +
draw_plot(var.plot, x = .5, y = .1, width = 0.6, height = 0.65)
figure
#Save plot
ggsave("Figure1_webh.tiff",figure,scale=1.2,width=25,height=12,units="cm",dpi=300)
